{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35820bc6",
   "metadata": {},
   "source": [
    "# LangChain: Fundamental\n",
    "\n",
    "Langchain is a framework for building applications with languange models. In this tutorial, we will learn the fundamental concepts of Langchain. \n",
    "\n",
    "1. Benefits of Langchain\n",
    "2. Basic Runnables in Langchain: Chatmodel, Prompt, Message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cda998",
   "metadata": {},
   "source": [
    "The benefits of LangChain are:\n",
    "\n",
    "* Easy switch between LLM providers: LangChain provides a unified interface for different LLM providers. Langchain exposes a standard interface for key components such as models, prompts, tool calling, output parsers,and chains.Therefore, it is easy for developers to switch between providers. It is mainly supported by langchain-core and langchain-community.\n",
    "\n",
    "* Easy to build applications with language models: combine multiple components and models into more complex applications, there’s a growing need to efficiently connect these elements into control flows that can accomplish diverse tasks. Orchestration is crucial for building such applications. It is mainly supported by langgraph.\n",
    "\n",
    "* Easy to track and debug: LangChain provides a platform for observability and evaluations. It is mainly supported by langsmith."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be50f5",
   "metadata": {},
   "source": [
    "### OpenAI API\n",
    "\n",
    "To run the following OpenAI model, you need to have the OpenAI API key. Here, we use the [environment variable](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety) `OPENAI_API_KEY` to store the API key. You can also set the API key in the code directly.\n",
    "\n",
    "```\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_api_key\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feefe107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Canada is Ottawa.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')\n",
    "chain = prompt | llm\n",
    "result = chain.invoke({\"input\": \"Hello, what is the capital of Canada?\"})\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f71d952",
   "metadata": {},
   "source": [
    "### Ollama\n",
    "\n",
    "To run the following code, you need to install Ollama and run the ollama locally. In the terminal, run the following command to start the ollama server:\n",
    "\n",
    "```\n",
    "ollama run qwen2.5:3b\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cead1f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Canada is Ottawa.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm_qwen = ChatOllama(model=\"qwen2.5:3b\")\n",
    "chain_qwen = prompt | llm_qwen\n",
    "result_qwen = chain_qwen.invoke({\"input\": \"Hello, what is the capital of Canada?\"})\n",
    "print(result_qwen.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f140d4df",
   "metadata": {},
   "source": [
    "\n",
    "### HuggingFace\n",
    "\n",
    "To run the following code, you need to install HuggingFace and run the HuggingFace server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6606e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Canada is Ottawa, located in the province of Ontario.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "llm_hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(max_new_tokens=256, \n",
    "                         do_sample=True, \n",
    "                         temperature=0.7, \n",
    "                         top_k=50, \n",
    "                         top_p=0.95,\n",
    "    ),\n",
    ")\n",
    "chat_model = ChatHuggingFace(llm=llm_hf)\n",
    "chain_hf = prompt | chat_model.bind(skip_prompt=True)\n",
    "result_hf = chain_hf.invoke({\"input\": \"Hello, what is the capital of Canada?\"})\n",
    "print(result_hf.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e407ab0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "llm_hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(max_new_tokens=256, \n",
    "                         do_sample=True, \n",
    "                         temperature=0.7, \n",
    "                         top_k=50, \n",
    "                         top_p=0.95,\n",
    "    ),\n",
    ")\n",
    "chat_model = ChatHuggingFace(llm=llm_hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebfb530",
   "metadata": {},
   "source": [
    "LangChain’s core abstraction is the *Runnable* interface, which provides a standard way to compose and execute language model chains. Via LCEL, runnables can be chained together using operators like (pipe) and + (combine), allowing you to build complex workflows from simple components. They support batch processing, streaming, async execution and other advanced features. The supported interfaces are as follows:\n",
    "| Interface | Description |\n",
    "| --- | --- |\n",
    "| Invoked | A single input is transformed into an output |\n",
    "| Batched | Multiple inputs are efficiently transformed into outputs |\n",
    "| Streamed | Outputs are streamed as they are produced |\n",
    "| Inspected | Schematic information about Runnable's input, output, and configuration can be accessed |\n",
    "| Composed | Multiple Runnables can be composed to work together using the LangChain Expression Language (LCEL) to create complex pipelines |\n",
    "\n",
    "And the major types of predefined runnables are as follows:\n",
    "\n",
    "| Component | Input Type | Output Type |\n",
    "| --- | --- | --- |\n",
    "| Prompt | dictionary | PromptValue |\n",
    "| ChatModel | a string, list of chat messages or a PromptValue | ChatMessage |\n",
    "| LLM | a string, list of chat messages or a PromptValue | String |\n",
    "| OutputParser | the output of an LLM or ChatModel | Depends on the parser |\n",
    "| Retriever | a string | List of Documents |\n",
    "| Tool | a string or dictionary, depending on the tool | Depends on the tool |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88b7e75",
   "metadata": {},
   "source": [
    "### ChatModel & LLM \n",
    "\n",
    "ChatModel is a wrapper around an LLM which provide a standardized way to interact with modern LLMs through a message-based interface. It should be preferred for development than Legacy LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8dbf138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tuesday.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI()\n",
    "response = llm.invoke(\"What is after Monday?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1da21ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Tuesday' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 12, 'total_tokens': 14, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-4c0d7a68-6458-4636-aa1a-cc1ba7a8addc-0' usage_metadata={'input_tokens': 12, 'output_tokens': 2, 'total_tokens': 14, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "chat = ChatOpenAI()\n",
    "response = chat.invoke(\"What is after Monday?\")\n",
    "print(response)\n",
    "# Returns: AIMessage(content='Tuesday')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb4d01",
   "metadata": {},
   "source": [
    "#### Prompt & Message\n",
    "\n",
    "1. Messages are the fundamental units of communication in chat models. They represent individual pieces of a conversation and have specific roles and content. \n",
    "2. Prompts are templates that help structure how we format inputs before sending them to language models.\n",
    "\n",
    "For message, it has four roles: Human, System, AI, and ToolMessage. \n",
    "\n",
    "HumanMessage: User inputs\n",
    "\n",
    "SystemMessage: Sets behavior/context for the AI\n",
    "\n",
    "AIMessage: Model responses\n",
    "\n",
    "ToolMessage: Results from tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a503e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "   \n",
    "messages = [\n",
    "       SystemMessage(content=\"You are a helpful assistant\"),\n",
    "       HumanMessage(content=\"What is LangChain?\")\n",
    "   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed64b8",
   "metadata": {},
   "source": [
    "Messages can help us to directly interact with ChatModels with the fine-grained control over conversion and specific conversion roles. And prompts can be beneficial for reusable templates, standardization inputs with variable contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0507508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "chat_template = ChatPromptTemplate([\n",
    "       (\"system\", \"You are a helpful assistant\"),\n",
    "       (\"user\", \"Tell me about {topic}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "585a9541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about AI', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(chat_template.invoke({\"topic\": \"AI\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297d77f1",
   "metadata": {},
   "source": [
    "In the above, we can see string is used to format the message. And we can also pass a list of messages to format the prompt. The example could be found as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b5413fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713d892a",
   "metadata": {},
   "source": [
    "The following code is an example of how to use the prompt and message in LangChain to create the few-shot prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0bc9881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful AI Assistant and you should use the examples to answer the question'), additional_kwargs={}), FewShotChatMessagePromptTemplate(examples=[{'input': 'what is 2~2', 'output': '4'}, {'input': 'what is 2~3', 'output': '6'}, {'input': 'what is 4~9', 'output': '36'}, {'input': 'what is 25~2', 'output': '50'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nAI: 16'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    FewShotChatMessagePromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "examples = [\n",
    "    {\"input\": \"what is 2~2\", \"output\": \"4\"},\n",
    "    {\"input\": \"what is 2~3\", \"output\": \"6\"},\n",
    "    {\"input\": \"what is 4~9\", \"output\": \"36\"},\n",
    "    {\"input\": \"what is 25~2\", \"output\": \"50\"},\n",
    "]\n",
    "example_prompt = ChatPromptTemplate(\n",
    "    [('human', '{input}'), ('ai', '{output}')]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    # This is a prompt template used to format each individual example.\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "final_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        ('system', 'You are a helpful AI Assistant and you should use the examples to answer the question'),\n",
    "        few_shot_prompt,\n",
    "        ('human', '{input}'),\n",
    "    ]\n",
    ")\n",
    "print(final_prompt)\n",
    "chain = final_prompt | llm\n",
    "chain.invoke({\"input\": \"what is 4~4?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
