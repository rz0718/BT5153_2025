{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This code demonstrates semantic similarity on sentence-level using BERT embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "aT7vplECtkJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import required libraries"
      ],
      "metadata": {
        "id": "xd9_kex-S-9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Eam08I10TcNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load pre-trained BERT model and tokenizer\n",
        "\n",
        "Here, **bert-base-uncased** is a good general-purpose model that converts text to lowercase"
      ],
      "metadata": {
        "id": "kS2dCcspTeiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# List of example sentences to compare\n",
        "# You can modify this list with your own sentences\n",
        "sentences = [\n",
        "    \"I love programming in Python\",\n",
        "    \"Python is my favorite programming language\",\n",
        "    \"The weather is beautiful today\",\n",
        "    \"I enjoy coding and software development\",\n",
        "    \"It's raining outside right now\",\n",
        "    \"Programming in Python is really fun\",\n",
        "]"
      ],
      "metadata": {
        "id": "ktMvxXZxTrPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize all sentences at once. The hyper-parameters are explained as:\n",
        "\n",
        "1. padding=True ensures all sequences have same length\n",
        "2. truncation=True cuts off sequences longer than max_length\n",
        "3. return_tensors=\"pt\" returns PyTorch tensors\n",
        "4. max_length=128 limits sequence length to 128 tokens"
      ],
      "metadata": {
        "id": "oTbuPMEOTuFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\n",
        "    sentences, padding=True, truncation=True, return_tensors=\"pt\", max_length=128\n",
        ")\n",
        "\n",
        "# Generate BERT embeddings for all sentences\n",
        "# torch.no_grad() disables gradient calculation for inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    # last_hidden_state contains contextual embeddings for each token\n",
        "    embeddings = outputs.last_hidden_state"
      ],
      "metadata": {
        "id": "QshYOWxzT_5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rf5S7gvDUJvT",
        "outputId": "ee7dc6e1-9b50-4161-fee2-b5e0d4a08378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 9, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get sentence embeddings by mean pooling, i,e, average out all of token embeddings in the sequence.\n",
        "\n",
        "This will conver the above shape from [6,9,768] to [6, 768]"
      ],
      "metadata": {
        "id": "Fz4HkNybUCWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "attention_mask = inputs[\"attention_mask\"]  # 1 for real tokens, 0 for padding\n",
        "# Expand attention mask to same dimensions as embeddings\n",
        "mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
        "# Apply mask to zero out padding token embeddings\n",
        "masked_embeddings = embeddings * mask\n",
        "# Sum all token embeddings for each sentence\n",
        "summed = torch.sum(masked_embeddings, 1)\n",
        "# Count number of real tokens in each sentence\n",
        "counts = torch.clamp(torch.sum(attention_mask, 1, keepdim=True), min=1e-9)\n",
        "# Calculate mean by dividing sum by count\n",
        "sentence_embeddings = (summed / counts).numpy()\n",
        "\n",
        "print(sentence_embeddings.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-vLiDfwSelP",
        "outputId": "b5a69e8c-9ed4-455c-e064-cd8842185778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform the query sentence to embeddings"
      ],
      "metadata": {
        "id": "C-xCyX6eUrn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define query sentence to compare against our sentence list\n",
        "query = \"I really enjoy Python programming\"\n",
        "print(f\"\\nQuery: {query}\\n\")\n",
        "\n",
        "# Process query sentence the same way as above\n",
        "# First, tokenize the query\n",
        "query_inputs = tokenizer(\n",
        "    [query], padding=True, truncation=True, return_tensors=\"pt\", max_length=128\n",
        ")\n",
        "\n",
        "# Generate BERT embeddings for query\n",
        "with torch.no_grad():\n",
        "    query_outputs = model(**query_inputs)\n",
        "    query_embeddings = query_outputs.last_hidden_state\n",
        "\n",
        "# Mean pooling for query embedding\n",
        "query_attention_mask = query_inputs[\"attention_mask\"]\n",
        "query_mask = query_attention_mask.unsqueeze(-1).expand(query_embeddings.size()).float()\n",
        "query_masked_embeddings = query_embeddings * query_mask\n",
        "query_summed = torch.sum(query_masked_embeddings, 1)\n",
        "query_counts = torch.clamp(torch.sum(query_attention_mask, 1, keepdim=True), min=1e-9)\n",
        "query_embedding = (query_summed / query_counts).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqDiwLQSTqHp",
        "outputId": "ada938d7-9ecf-44cf-93cb-3d5eb7ccea9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: I really enjoy Python programming\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate cosine similarity between query and all sentences\n",
        "1. Cosine similarity measures the cosine of the angle between two vectors\n",
        "2. Values closer to 1 indicate higher similarity"
      ],
      "metadata": {
        "id": "-0jJWHdmU_MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarities = cosine_similarity(query_embedding, sentence_embeddings)\n",
        "\n",
        "# Sort sentences by similarity score (highest to lowest)\n",
        "similar_sentence_indices = similarities[0].argsort()[::-1]\n",
        "\n",
        "# Print results\n",
        "print(\"Most similar sentences (in order of similarity):\")\n",
        "for idx in similar_sentence_indices:\n",
        "    print(f\"Similarity: {similarities[0][idx]:.4f} - {sentences[idx]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIUKHJftU_zQ",
        "outputId": "4ce2149b-08c5-4030-f240-d70ca25b5ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar sentences (in order of similarity):\n",
            "Similarity: 0.8981 - I love programming in Python\n",
            "Similarity: 0.8463 - I enjoy coding and software development\n",
            "Similarity: 0.7940 - Python is my favorite programming language\n",
            "Similarity: 0.7896 - Programming in Python is really fun\n",
            "Similarity: 0.5652 - The weather is beautiful today\n",
            "Similarity: 0.5038 - It's raining outside right now\n"
          ]
        }
      ]
    }
  ]
}