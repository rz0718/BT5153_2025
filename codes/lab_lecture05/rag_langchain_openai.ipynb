{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265d8984",
   "metadata": {},
   "source": [
    "#### Building a RAG System with LanceDB and OpenAI\n",
    "This notebook demonstrates how to build a Retrieval Augmented Generation (RAG) system using:\n",
    "- LanceDB for vector storage\n",
    "- OpenAI for text generation\n",
    "- LangChain for the RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3267e4",
   "metadata": {},
   "source": [
    "#### 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "793cf511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rz/Documents/projects_code/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import LanceDB\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import lancedb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b7ed1",
   "metadata": {},
   "source": [
    "#### 2. Initialize Embedding Model\n",
    "We use HuggingFace's all-mpnet-base-v2 model for generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13cf342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3e6ce",
   "metadata": {},
   "source": [
    "#### 3. Load and Process PDF Documents\n",
    "Load PDF files from the specified directory and split them into manageable chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a6f5d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF documents...\n"
     ]
    }
   ],
   "source": [
    "# Directory containing PDF files\n",
    "pdf_directory = \"offline_doc\"\n",
    "\n",
    "# Load PDF documents\n",
    "print(\"Loading PDF documents...\")\n",
    "documents = []\n",
    "for filename in os.listdir(pdf_directory):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(pdf_directory, filename)\n",
    "        loader = PyPDFLoader(file_path) # it will have the metadata of the pdf flie and its content. here, the metadata is the filename and the page number\n",
    "        documents.extend(loader.load())\n",
    "\n",
    "# Split documents into chunks\n",
    "# RecursiveCharacterTextSplitter is a text splitter that splits the text into chunks of a specified size, with a specified overlap\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "processed_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da89b441",
   "metadata": {},
   "source": [
    "#### 4. Create Vector Store\n",
    "Initialize LanceDB and store document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebc964d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store...\n"
     ]
    }
   ],
   "source": [
    "# Initialize LanceDB\n",
    "print(\"Creating vector store...\")\n",
    "db = lancedb.connect(\"lancedb\")\n",
    "\n",
    "# Extract data\n",
    "texts = [doc.page_content for doc in processed_docs]\n",
    "metadatas = [doc.metadata for doc in processed_docs]\n",
    "\n",
    "# Get embeddings\n",
    "embeddings_list = embeddings.embed_documents(texts)\n",
    "\n",
    "# Create data for the table\n",
    "# For each context, we create a row in the table with the following columns: id, vector, text, and metadata\n",
    "data = [\n",
    "    {\"id\": str(i), \"vector\": emb, \"text\": text, \"metadata\": metadata}\n",
    "    for i, (emb, text, metadata) in enumerate(zip(embeddings_list, texts, metadatas))\n",
    "]\n",
    "\n",
    "# Create table\n",
    "table = db.create_table(\"pdf_vectors\", data=data, mode=\"overwrite\")\n",
    "\n",
    "# Create vector store\n",
    "vector_store = LanceDB(\n",
    "    connection=db,\n",
    "    table_name=\"pdf_vectors\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfe7d6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 1, 'source': 'offline_doc/group02.pdf'}, page_content='Detection of AI-Generated Text for Essay Competitions \\n \\n2 | P a g e  \\n \\nprocessed to support the development and evaluation of \\nmachine learning models . The data was broadly \\ncategorized into general text  and real competition essay \\nsubmissions, each contributing uniquely to the study. \\nThe general text category included a dataset from Kaggle, \\nfeaturing 29,145 samples of student essays and \\nGPT(Curie)-generated essays on car -free cities. On the \\nother hand , the Wiki Introduction dataset, sourced from \\nHugging Face, consisted of 150,000 pairs of Wikipedia \\nintroductions and their AI -generated versions. Both \\ndatasets were utilized for preliminary finetuning  of all \\ncandidate models , among w hich the best combination of \\nmodeling features will be chosen for subsequent finetuning.  \\nUnder the competition essay category, an essay dataset  \\nincluded 50 past winning essays from five global \\ncompetitions matched with GPT-3.5/4.0 generated content'),\n",
       " Document(metadata={'page': 8, 'source': 'offline_doc/group02.pdf'}, page_content='Detection of AI-Generated Text for Essay Competitions \\n \\n9 | P a g e  \\n \\nComputational Linguistics, Seattle, United States, \\n1213–1233. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAppendix 1. Figures for EDA \\n \\n  \\nFigure 3 Histogram of Word Count in AI-generated (AIG) and Non-AIG \\nTexts in New_Essay, Kaggle, Wiki Datasets respectively  \\n \\n \\n  \\nFigure4  Histogram of Sentence Variation in AIG and Non-AIG Texts in \\nNew_Essay, Kaggle, Wiki Datasets respectively \\n \\n \\n  \\nFigure 5 Histogram of Word Length for AIG and Non-AIG Texts in \\nNew_Essay, Kaggle, Wiki Datasets respectively'),\n",
       " Document(metadata={'page': 2, 'source': 'offline_doc/group02.pdf'}, page_content='Detection of AI-Generated Text for Essay Competitions \\n \\n3 | P a g e  \\n \\n3. Percentage of stop words per text  (word count): \\nReflecting sentence structure, syntax, and language \\nfluency, this feature offers valuable insights into the \\ncomposition of the text.  \\n4. Percentage of adjectives per text  (word count): This \\nfeature serves as an indicator of rich description, \\nexpressive language, and subjective interpretation \\nwithin the text samples. \\nBy incorporating these feature s, we aim  to capture \\nmeaningful linguistic characteristics of the text data and \\nenhance the model’s capability to detect the AI-generated \\ntext. \\nOnce the features are extracted , all text is converted to \\nlowercase to standardize the text format and facilitate \\nuniform processing. Punctuation marks are removed from \\nthe text to focus solely on the textual content. Stop words, \\nwhich are  common words like “the”, “ and”, “ is” are \\nremoved to reduce noise and improve the relevance of the')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create retriever\n",
    "# Top-k is the number of chunks to retrieve\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "retriever.invoke(\"What is the dataset prepared to detect the AI-generated essays?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884bf8be",
   "metadata": {},
   "source": [
    "#### 5. Create RAG Chain\n",
    "Set up the retrieval and generation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff95e7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RAG chain...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating RAG chain...\")\n",
    "# Initialize LLM (OpenAI)\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "# Create prompt template\n",
    "template = \"\"\"Use the following pieces of context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "# Format documents function \n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"{doc.page_content}\\n(Source: {doc.metadata['source']}, Page: {doc.metadata['page']})\"\n",
    "        for doc in docs\n",
    "    )\n",
    "# Create the RAG chain\n",
    "# RunnablePassthrough() is used to pass the question through the chain unchanged. It means that the question firstly pass through the retriever\n",
    "# then format the context. And than the question would be passed with the retrieved context to the prompt\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# StrOutputParser() is used to parse the output of the llm to a string.\n",
    "# Here, it is the ChatModel. So it extracts the .content attribute of the message, ensuring that the final output is in string format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8521f72e",
   "metadata": {},
   "source": [
    "### Ask questions about the PDF content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "240cde72",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'How is the dataset prepared to detect the AI-generated essays?'\n",
    "response = rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c644ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset for detecting AI-generated essays is prepared by categorizing the data into two main types: general text and real competition essay submissions. The general text category includes a dataset from Kaggle with 29,145 samples of student essays and GPT(Curie)-generated essays on car-free cities, as well as the Wiki Introduction dataset from Hugging Face, which consists of 150,000 pairs of Wikipedia introductions and their AI-generated versions. These datasets are used for preliminary finetuning of candidate models. The competition essay category includes a dataset of 50 past winning essays from five global competitions matched with GPT-3.5/4.0 generated content.\n",
      "\n",
      "To enhance the model's capability to detect AI-generated text, specific linguistic features are extracted, such as the percentage of stop words and adjectives per text. The text is standardized by converting it to lowercase, and punctuation marks and stop words are removed to focus on the textual content and reduce noise.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5a29dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
